{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cac53cf9-7bfa-4da8-9506-c2dba2dda92e",
   "metadata": {},
   "source": [
    "Lab 2 : Open AI Gymnasium\n",
    "----\n",
    "\n",
    "As we are steadily advancing towards Reinforcement Learning in class, it's time to get our hands dirty with another cool Python package. Gymnasium, originally developed by OpenAI is a useful tool that is commonly used for solving RL-related challenges. With Gymnasium, users can easily design, implement, and evaluate reinforcement learning algorithms by leveraging predefined environments such as classic control tasks, Atari 2600 games, and more. Furthermore, Gymnasium provides an easy API to implement your own environments!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52ed948-05a8-4a3f-9a66-c2a754c8b329",
   "metadata": {},
   "source": [
    "First, let's get started with installing Gymnasium!\n",
    "\n",
    "Step 1. Make sure you have python and pip installed. Check the installation of python and pip using \n",
    "- `python --version` and `pip --version`\n",
    "If pip is not installed:\n",
    "- Follow the steps listed for your respective operating system at [Pip Docs - Installation](https://pip.pypa.io/en/stable/installation/).\n",
    "\n",
    "Step 2. Now execute `pip install gym` in your terminal to install gymnasium or run the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51262448",
   "metadata": {},
   "source": [
    "`!pip install gym`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccaff9a-c81d-4889-9236-b65341252f47",
   "metadata": {},
   "source": [
    "Now to verify installation of gym execute the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af7d2e68-590a-450e-8e64-a031afabe535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3c4595-a2f2-4995-a65f-74d48bb57b48",
   "metadata": {},
   "source": [
    "Great! Now let's talk about environments! \n",
    "\n",
    "The fundamental building block of OpenAI Gym is the `Env` class. It is a Python class that implements a simulator that runs the  environment you want to train your agent in. Open AI Gym comes packed with a lot of environments, such as one where you can move a car up a hill, balance a swinging pendulum etc. Take a moment now to visit [Gymnasium - Farama](https://gymnasium.farama.org/) to see the complete list of environments, along with demonstrations of what the task in each environment is. \n",
    "\n",
    "In this lab, we will begin with the `MountainCar` environment. Our goal is to control a car on a track positioned between two mountains. The objective is to drive up the mountain on the right, but the car's engine lacks the power to climb it directly. To solve this problem, we require the car to make strategic back-and-forth movements to build momentum. \n",
    "\n",
    "Let's see how we can train our car to climb up the mountain using gymnasium."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c64f01-9657-4071-a037-71919d671119",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb0da5d9-ba4b-4764-accf-1ca1e05948f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# While creating our mountain car environment, render_mode is an optional argument. \n",
    "# It is used to render the environment visually when needed.\n",
    "# For more details, visit https://gymnasium.farama.org/api/env/\n",
    "\n",
    "env = gym.make('MountainCar-v0', render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be7abdf-d9b9-42b4-9a8c-2efa1997704d",
   "metadata": {},
   "source": [
    "In Reinforcement Learning, we think of two key terms - the `Observation Space` and `Action space`.\n",
    "\n",
    "The `Observation Space` is the set of all possible observations that an agent can receive from the environment. These observations provide information about the current state of the environment. Observations can be diverse and may include sensor readings, images, or any relevant data that helps the agent make decisions.\n",
    "\n",
    "For example, for an agent trying to learn how to shoot a target the observation space can include - the agent's current position or location, the position and movement of the target, information about obstacles or barriers in the environment if any, and the agent's ammunition status or the number of bullets remaining.\n",
    "\n",
    "The `Action Space` represents the set of possible actions that the agent can take to interact with the environment. These actions represent the decisions or movements that the agent can make to interact with the environment.\n",
    "\n",
    "For example for an agent trying to learn how to shoot a target the action space can include - adjusting the aim or direction of the firearm, pulling the trigger to shoot, reloading the firearm, and changing the stance or position of the agent.\n",
    "\n",
    "The basic structure of the environment is described by the observation_space and the action_space attributes of the Gym Env class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480cd8d0-0dfc-4d21-b5f4-48f74d1bb7a6",
   "metadata": {},
   "source": [
    "## To-do \n",
    "Can you guess what the observation space and state space of the mountain car problem would be? Write your answer below. Try to provide 2-3 examples for each.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737a3565",
   "metadata": {},
   "source": [
    "Answer: For a car trying to climb up the mountain, the observation space would include the elevation of the car and the speed of the car (negative for when it is moving away from the top of the mountain and positive when it is moving towards).\n",
    "\n",
    "For the action space, the mountain car can only move forward or backwards to get enough momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d161a237-b07c-4228-8f41-e422a45ccc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The observation space: Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
      "The action space: Discrete(3)\n"
     ]
    }
   ],
   "source": [
    "# Observation and action space for mountain-view problem\n",
    "obs_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "print(\"The observation space: {}\".format(obs_space))\n",
    "print(\"The action space: {}\".format(action_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6e1260-0405-497b-b1a8-3c4033200966",
   "metadata": {},
   "source": [
    "The observation_space for our environment is a Box with shape (2,), and returns two lists with entries of type float32.\n",
    "The action_space was Discrete with shape (2). \n",
    "\n",
    "What do these actually mean? \n",
    "\n",
    "Both Box and Discrete are types of data structures called \"Spaces\" provided by Gym, modeling various aspects of the problem at hand. All of these data structures are derived from the gym.Space base class.\n",
    "\n",
    "Box, for instance, is used when modeling real-valued quantities, i.e. a continuous space. The printed output can be a bit tricky to read - the first list in the observation space above contains minimum values for the set of quantities that are modeled, and the second list contains the maxima. \n",
    "\n",
    "For this particular problem (Mountain Car), the values at index 0 within both lists refer to the minimum and maximum value of the car's position along the x-axis, i.e. the car will always be between x=-1.2 and x=0.6. The values at index 1 represent the range of the velocity, i.e. the car's speed will be between -0.07 and 0.07. Both these values are allowed to vary continuously with the precision of the float32 dtype.\n",
    "\n",
    "Discrete, on the other hand, signifies that there are a finite number of options to choose from. In this case, Discrete(3) represents the fact that the agent has three choices: 0: accelerate left, 1: don't accelerate, and 2: accelerate right. The easiest way to find descriptions for actions (and their indices) is to visit the [Gymnasium page for that task](https://gymnasium.farama.org/environments/classic_control/mountain_car/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65911af6-c32d-45bc-aa78-a5257e3b9195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upper Bound for Env Observation [0.6  0.07]\n",
      "Lower Bound for Env Observation [-1.2  -0.07]\n"
     ]
    }
   ],
   "source": [
    "print(\"Upper Bound for Env Observation\", env.observation_space.high)\n",
    "print(\"Lower Bound for Env Observation\", env.observation_space.low)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca0d771-dee2-4571-b337-1abb978f9549",
   "metadata": {},
   "source": [
    "You can set these upper/lower limits while defining your space, as well as when you are creating an environment.\n",
    "\n",
    "From here on, remember that an observation for the mountain car environment is a vector of two numbers, representing position and velocity respectively. The middle point between the two mountains in the environment is taken to be the origin, with the right being considered the positive direction and the left being the negative direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02de71b-4878-4222-ba60-262972798b87",
   "metadata": {},
   "source": [
    "Now let's talk about how the env class helps the agent interact with the environment!\n",
    "\n",
    "`reset`: This function returns the initial observation of the environment after placing the agent back in its starting state. Our first observation in any RL task implemented using Gymnasium must be obtained by calling this function. This function should also be called every time a terminal/end state is reached.\n",
    "`step`: This function takes an action as input, applies it to the environment, and returns the following:\n",
    "- `observation`: The current state of the environment after the action is taken.\n",
    "- `reward`: The reward obtained from the action.\n",
    "- `terminated`: Return true or false, depending on whether the agent has reached a terminal state (as defined under the task).\n",
    "- `truncated`: Determine if the truncation condition, often a time limit or agent going out of bounds, is satisfied. This can prompt an early episode termination before reaching a terminal state. Think of this as cases that may end a game preemptively.\n",
    "- `info`: Extra information for debugging or environment-specific details, such as lives remaining.\n",
    "\n",
    "Next, we demonstrate the step function, which is how we get the agent to execute an action in a Gymnasium environment. For now, we will simply pick a random action from the environment's action space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1898f7c-34f7-4501-abb2-acc459a90bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial observation is (array([-0.5548162,  0.       ], dtype=float32), {})\n",
      "The new observation is [-0.5555824  -0.00076621]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment and see the initial observation\n",
    "obs = env.reset()\n",
    "print(\"The initial observation is {}\".format(obs))\n",
    "\n",
    "# Sample a random action from the entire action space\n",
    "random_action = env.action_space.sample()\n",
    "\n",
    "# # Take the action and get the new observation space\n",
    "new_obs, reward, terminated, truncated, info = env.step(random_action)\n",
    "print(\"The new observation is {}\".format(new_obs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793cbd70",
   "metadata": {},
   "source": [
    "This step is optional and only needs to be executed if you want to render and view the environment. May be finicky on MacOS. It's fine to skip it for now!\n",
    "\n",
    "- `pip uninstall -y pygame`\n",
    "- `pip install pygame --pre`\n",
    "- `pip install gym[classic_control]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28d9d259-6933-420a-8201-a536429bec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only way to end a simulation\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f1cd30-8d74-4d44-8bbf-422034a674af",
   "metadata": {},
   "source": [
    "# To-do\n",
    "Now it's your turn to try writing code to see how far up the hill this cart can go by taking random steps in this environment.\\\n",
    "Since we are not training the agent at all, we don't expect to see much progress, but this should give us a good idea of how gymnasium works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e95af7-3928-4a67-96f0-042eb1813e73",
   "metadata": {},
   "source": [
    "#### To Ponder:\n",
    "\n",
    "In the previous code block where we were taking actions, we didn't use the agent's state and reward to decide the best action from the new state. All the generated actions are random, and you'll be doing the same thing below. We'll learn how to train agents in lecture!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cef06417-1720-42fd-9045-0cef49e52bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "env_screen= None\n",
    "\n",
    "# Number of steps you run the agent for\n",
    "# This may take a few minutes to execute depending on the number of steps\n",
    "num_steps = 2000\n",
    "\n",
    "# reset the environment here\n",
    "env.reset()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # Render the env here (Rendering might take longer for the cell to execute. \n",
    "    # Stop execution once you get a sense of what's happening and move on to the next part of the lab).\n",
    "    plt.imshow(env.render())\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True) \n",
    "    \n",
    "    # take random action (based on examples above)\n",
    "    random_action = env.action_space.sample()\n",
    "    \n",
    "    # apply the action using env.step here\n",
    "    new_obs, reward, terminated, truncated, info = env.step(random_action)\n",
    "    \n",
    "    # If the episode is finished (i.e., terminated or truncated), then reset the env and start another one;\n",
    "    if terminated or truncated:\n",
    "        obs, info = env.reset()\n",
    "        break\n",
    "\n",
    "# Close the env\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8741d2e0-62c9-4884-92ea-c7eec4d23900",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "Now that we have seen how to use gymnasium using its default environment, let's see how can we build a custom environment and perform actions in it. This particular custom environment is built using the gymnasium extension for Markov Decision Processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b744f4c8-a757-45a1-9e64-627d1d88a589",
   "metadata": {},
   "source": [
    "Let's consider a 3x3 grid where the action space is to move up, down, left, or right. An action that takes you out of the grid is considered invalid. The red cells in the grid represent a reward of -20, the yellow cell has a reward of +5 and the green one has a reward of +10. Consider the green cell as the final goal state, which is where we want our agent to go. There is an equal probability of taking any valid action from a given cell. \n",
    "\n",
    "Here's how the grid is defined **(on a Euclidean plane)**:\n",
    "\n",
    "Cell 0 - 0,0 (start)\\\n",
    "Cell 1 - 0,1\\\n",
    "Cell 2 - 0,2 (red)\\\n",
    "Cell 3 - 1,0\\\n",
    "Cell 4 - 1,1\\\n",
    "Cell 5 - 1,2 (yellow)\\\n",
    "Cell 6 - 2,0 (goal)\\\n",
    "Cell 7 - 2,1\\\n",
    "Cell 8 - 2,2 (red)\n",
    "\n",
    "<div><img src=\"./grid.png\", style=\"width:400px\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e90bab",
   "metadata": {},
   "source": [
    "`pip install matrix-mdp-gym`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be1e5434-274a-4e43-8424-a8543f97b420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_states = 9 # total number of cells\n",
    "num_actions = 4 # up, down, left, right\n",
    "num_terminal_states = 1 # initilaise the number of terminal goal states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5758daba-44c3-4f45-b859-7f5958185ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To-do:\n",
    "# Initialise the action map as a dictionary, just to save human readable labels. 0 maps to 'up', 1 to 'down', 2 to 'left', and 3 to 'right'.\n",
    "A = {0:'up', 1:'down', 2:'left', 3:'right'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5fe8bc81-5b56-498d-bea9-41891a5f5674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To-do: fill in and complete the state_action_map here, this is a map of all the valid actions from a given state. \n",
    "# Values for cell 0 and 6 are filled in for you\n",
    "# (from cell 0 we can go to the cell above it, or the cell to its right. \n",
    "# The action map values, therefore, are 0 and 3. For cell 6, since this state is terminal, there are no actions from it.\n",
    "\n",
    "# While we are hardcoding this for now, you should consider writing code \n",
    "# that generates this map using the rules of your environment in general.\n",
    "\n",
    "states_actions_map = {\n",
    "    0: [0, 3],\n",
    "    1: [0, 2, 3],\n",
    "    2: [0, 2],\n",
    "    3: [0, 1, 3],\n",
    "    4: [0, 1, 2, 3],\n",
    "    5: [0, 1, 2],\n",
    "    6: [], # terminal\n",
    "    7: [1, 2, 3],\n",
    "    8: [1, 2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3a1f95d0-b87f-4737-b64f-29ada8813b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize transition and reward matrices.\n",
    "\n",
    "T = np.zeros((num_states, num_states, num_actions), dtype='int64')\n",
    "R = np.zeros((num_states, num_states, num_actions), dtype='int64')\n",
    "\n",
    "# 2. Fill in the transition matrix with the correct values.\n",
    "\n",
    "    # The matrix T represents transition probabilities, i.e., the probability of moving to state j from state i using action a. \n",
    "    # In uncertain environments, an action may have a small probability of failing and leading you to a different state than intended.\n",
    "    # For now, assume that the probability of failure is 0, i.e., an up action, for instance, \n",
    "    # will always take you to the cell above your current state, as long as such a cell exists. \n",
    "\n",
    "    # Note that T is a 3-dimensional matrix. Entries should be made at the index [new_state][current_state][action].\n",
    "    # For example, T[1][0][3] = 1 means that if we take action 3 (right) from cell 0 (start), the probability of reaching the cell 1 is 1.\n",
    "    # Use numpy operations to fill in the correct values in the matrix T.\n",
    "\n",
    "# check if the next direction this cell is taking is a valid direciton\n",
    "def check_state_action_map(curr_cell_number, direction):\n",
    "    return direction in states_actions_map.get(curr_cell_number, [])\n",
    "\n",
    "# check if we can travel to that next cell depending on the direction we take\n",
    "def check_next_cell(next_cell_number, curr_cell_number, direction):\n",
    "    if direction == 0: # up\n",
    "        return (next_cell_number - curr_cell_number == 3)\n",
    "    \n",
    "    elif direction == 1: # down\n",
    "        return (curr_cell_number - next_cell_number == 3)\n",
    "    \n",
    "    elif direction == 2: # left\n",
    "        return (curr_cell_number - next_cell_number == 1)\n",
    "    \n",
    "    elif direction == 3: # right\n",
    "        return (next_cell_number - curr_cell_number == 1)\n",
    "    \n",
    "    return False\n",
    "\n",
    "for next_cell_number in range(0, 9):\n",
    "    for current_cell_number in range(0, 9):\n",
    "        for action_number in range(0, 4):\n",
    "            if (check_state_action_map(current_cell_number, action_number) == True) and (check_next_cell(next_cell_number, current_cell_number, action_number)):\n",
    "                T[next_cell_number][current_cell_number][action_number] = 1\n",
    "            else:\n",
    "                T[next_cell_number][current_cell_number][action_number] = 0\n",
    "\n",
    "print(T[6][6][3])\n",
    "\n",
    "# 3. Fill in the reward matrix with the correct values.\n",
    "\n",
    "    # The matrix R represents the rewards obtained by moving to state j from state i using action a. \n",
    "    # The red cells in the grid have a reward of -20, the yellow cell has a reward of +5, and the green one has a reward of +10.\n",
    "    # Assume rewards don't depend on the action used or the cell from which the agent moved to one of these cells for now.\n",
    "\n",
    "    # Entries should once again be made at the index [new_state][current_state][action].\n",
    "    # For example, R[2][1][3] = -20 indicates that if we take action 3 from cell 1 and reach cell 2, the reward is -20.\n",
    "    # Update the entries in the matrix R with the correct values below. \n",
    "\n",
    "def reward_cells(next_cell_num):\n",
    "    if (next_cell_number == 2) or (next_cell_number == 8):\n",
    "        return -20\n",
    "    \n",
    "    elif (next_cell_number == 5):\n",
    "        return 5\n",
    "    \n",
    "    elif (next_cell_number == 6):\n",
    "        return 10\n",
    "    \n",
    "    return 0\n",
    "\n",
    "for next_cell_number in range(0, 9):\n",
    "    for current_cell_number in range(0, 9):\n",
    "        for action_number in range(0, 4):\n",
    "            if (T[next_cell_number][current_cell_number][action_number] == 1):\n",
    "                R[next_cell_number][current_cell_number][action_number] = reward_cells(next_cell_number)\n",
    "\n",
    "            else:\n",
    "                R[next_cell_number][current_cell_number][action_number] = 0\n",
    "\n",
    "print(R[6][3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6aa0769d-4d9a-461f-beb0-1e00cf8dd1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P_0 is simply the initial probability distribution. This represents where your agent is at the beginning.\n",
    "# If there is only one possible start state, the probability distribution is simply a one-hot vector,\n",
    "# where the probability at the start state is 1 and 0 elsewhere.\n",
    "\n",
    "import numpy as np\n",
    "''' Write Start state '''\n",
    "P_0 = np.array([1, 0, 0, 0, 0, 0, 0, 0, 0], dtype='int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "515f2f0d-09b1-4bad-9163-73c901ce7d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matrix_mdp\n",
    "import gymnasium as gym\n",
    "env = gym.make('matrix_mdp/MatrixMDP-v0', p_0=P_0, p=T, r=R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ca894307-9976-4215-bc81-c751fc00d66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial observation is 0\n"
     ]
    }
   ],
   "source": [
    "#First, we reset the environment and get the initial observation.\n",
    "observation, info = env.reset()\n",
    "print(\"The initial observation is {}\".format(observation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07c9c72-19ec-4995-828f-a74b7a32ed3b",
   "metadata": {},
   "source": [
    "Below, write the code for random exploration, i.e. randomly choosing an action at each time step and executing it.\n",
    "\n",
    "A random action is simply a random integer between 0 and the number of actions (num_actions not inclusive).\n",
    "However, you should make sure that the chosen action can actually be taken from the current state (i.e., the chosen action is valid).\n",
    "\n",
    "Keep track of the total reward in each episode, and reset the environment when the episode terminates.\n",
    "\n",
    "Print the average reward obtained over 10000 episodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "3f2d1662-ba33-4f01-aec6-88be1cfa8c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting total reward as zero\n",
    "total_rewards = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1d33a4a4-8b6b-452e-bc71-ec6adfdda8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# To-Do:\n",
    "# For each episode, collect the reward the agent earns and store it.\n",
    "# After the end of all 10000 episodes, we are going to take the average reward earned by the agent.\n",
    "\n",
    "num_of_episodes = 1\n",
    "reward_list = []\n",
    "\n",
    "for i in range(num_of_episodes):\n",
    "    observation, info = env.reset()\n",
    "    total_rewards = 0\n",
    "\n",
    "    while True:\n",
    "        # Pick an action using the random Python module random until we have a valid action for that cell\n",
    "        # Hint: use states_actions_map in conjunction with the random module\n",
    "        random_action = env.action_space.sample()\n",
    "        \n",
    "        # Get a new observation, reward, terminated and other information here using the env.step() function\n",
    "        new_obs, reward, terminated, truncated, info = env.step(random_action)\n",
    "        \n",
    "        # Update rewards\n",
    "        ''' Write Code '''\n",
    "\n",
    "        # break this loop if we have reached an end state\n",
    "        if ''' Terminate condition ''':\n",
    "            break\n",
    "        \n",
    "        reward_list.append(total_rewards)\n",
    "\n",
    "# calculate and print the average reward here\n",
    "avg_reward = \n",
    "print(\"Average reward obtained: \", avg_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b22b01-2c98-4ad3-b9c6-6f1d9416868d",
   "metadata": {},
   "source": [
    "Hope this lab helped you understand how to set up and navigate an environment in Gymnasium. \n",
    "As always, please don't hesitate to reach out to the instructor or the TAs if you have any questions!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
